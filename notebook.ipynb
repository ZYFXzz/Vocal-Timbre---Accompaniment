{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a test notebook for idea proofing of my current thesis idea on elevating current Singing VOice synthesis system. It is investigating how vocal timbre(dynamics, tension.......etc) relates to changes in its accompaniment.\n",
    "\n",
    "Other notes/inspiration :\n",
    "0. the idea first started as the broad goal -  to add more expression to current SVS system. I tried to narrowed it down throughout the first month of this fall semester, what I come up with, is try use an accompaniment, as another source of controls. The interface for music production/ music generation is evolving today, a notable advancement is new natural language interface with Large Language Models like GPT. Some latest research of music generation incorporates audio as another source for model interference/controls. I was thinking about things like this, is to use audio as a control source for SVS. \n",
    "\n",
    "1. In many published SVS research, the evaluation is subjective listening test on naturalness of the synthesized sounds. Human tester were usually given some clips, around 10s each, to evaluate the naturalness of the voice, and may compare it to a ground truth human singing.\n",
    "\n",
    "2. However, in contrast to point 1 above, generating 2-3 minutes of singing voice is more likely to happen instead of 10s clips.In most user cases, a user(music producer) may want to generate singing for a full-length song that lasts for 2-4 minutes. As a long-time user of SVS applications, sometimes it felt short for me when generating expressive vocal that aligns with emotions/dynamics of the accompaniment. \n",
    "\n",
    "3. SVS has continuing development of industrial application, Yamaha has released VOCALOID together with Music Technology Group from UPF, Spain. The interface still looks similar from the first day it came out, which includes a piano roll for midi and lyrics input, and a curve parameter window for advanced user controls. It has been a headache for both inputting basic midi plus lyrics info and drawing those curved parameters for users. It is potentially part of the reason why Vocaloid and similar software did not gain large interest from users. People who mainly use it during production are producers from Japan. \n",
    "\n",
    "3. It is notable in an music ensemble environment, or any sort of music collaboration that involves multiple human player or instruments\n",
    "\n",
    "4. The current thesis idea, as by today 10/16/2024, is to automate the curve parameter (energy/tension) available in OpenUtau Editor with DiffSinger singing voice model, based on the RMS level of an accompaniment track. \n",
    "\n",
    "5. To support the idea of point 4, this notebook or repo serves to explore the possible relationship in between singing and accompaniment, by looking at some available music dataset with available stems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this exploration, following tools/dataset will be used:\n",
    "\n",
    "- **mirdata** - for easy working with dataset like medley-DB\n",
    "\n",
    "- **librosa** - for all audio processing, feature extractions\n",
    "\n",
    "- **Medley DB** - the audio source\n",
    "\n",
    "**some codes are copied directly from homework of this semester course - Music Information Retrieval, as those notebook provide good example of working with tools and data in notebook and python environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "import mirdata\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setups\n",
    "\n",
    "# This two lines of code make your notebook aware of changes in\n",
    "# your utils.py file without needing to restart the session each time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acousticbrainz_genre', 'baf', 'beatles', 'beatport_key', 'billboard', 'candombe', 'cante100', 'cipi', 'compmusic_carnatic_rhythm', 'compmusic_carnatic_varnam', 'compmusic_hindustani_rhythm', 'compmusic_indian_tonic', 'compmusic_jingju_acappella', 'compmusic_otmm_makam', 'compmusic_raga', 'da_tacos', 'dagstuhl_choirset', 'dali', 'egfxset', 'filosax', 'four_way_tabla', 'freesound_one_shot_percussive_sounds', 'giantsteps_key', 'giantsteps_tempo', 'good_sounds', 'groove_midi', 'gtzan_genre', 'guitarset', 'haydn_op20', 'idmt_smt_audio_effects', 'ikala', 'irmas', 'maestro', 'medley_solos_db', 'medleydb_melody', 'medleydb_pitch', 'mridangam_stroke', 'mtg_jamendo_autotagging_moodtheme', 'openmic2018', 'orchset', 'phenicx_anechoic', 'queen', 'rwc_classical', 'rwc_jazz', 'rwc_popular', 'salami', 'saraga_carnatic', 'saraga_hindustani', 'scms', 'slakh', 'tinysol', 'tonality_classicaldb', 'tonas', 'vocadito']\n"
     ]
    }
   ],
   "source": [
    "print(mirdata.list_datasets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "medley = mirdata.initialize('medleydb_pitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "downgrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
